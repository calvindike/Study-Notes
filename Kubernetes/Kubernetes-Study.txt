Getting Started with Kubernetes - Nigel Poulton
At the highest level, kubernetes is an orchestrator of microservices apps.
Kubernetes is a container orchestration platform which is the main platform for managing containers at scale.
It was introduced by Google in 2015. This first had the name 'Seven of Nines' but couldn't continue with that name due to copyright reasons. The 7 spokes in the icon is a tribute to the name.
Kubernetes is often shortened to K8s as the 8 letters 'ubernete' = 8

Containers and microservices bring a whole new set of management challenges. Legacy/Monolith apps used hundreds of servers but modern microservice apps use several thousands of containers. Kubernetes is seen as the OS of the cloud as it serves as that abstraction layer when we're trying to run an application.
We package apps as containers which are described in a manifest file and given to Kubernetes to handle the rest.

Monolith apps have all services in it bundled into one but in microservice architecture, all those services are packaged as their own app so we do NOT need to shutdown all services in the app just to patch one section of the app. These microservice apps now run on multiple containers according to scale.
Cloud native apps have to be able to run anywhere we have Kubernetes. There are public cloud, hybrid app, and on prem.

-------Kubernetes Architecture--------
An example of what K8s will do for us is to organize and connect the different microservice apps we have into a functional application - this is called orchestration. K8s also reacts to real-time events because say a node fails, k8s will spin up a replacement to make sure all is running as it should.
In a cluster, Control Plane which are in the control plane nodes are the brains of the cluster which does scheduling tasks and monitoring of events in the cluster.
The workers/worker nodes are where we run our user and business apps.
We would take the app code, and containerize it. This app is wrapped in a pod, which is wrapped in a deployment. All these are described in a yaml file. 
app code in a container, container in a pod, pod in a deployment

------Control Plane Nodes-------
This is the brains of Kubernetes and has to be Linux. These used to be called masters but Kubernetes replaced this with a less suggestive name (think master and slave).
When having control nodes, 3 or 5 control nodes are the sweet spot as you always want a majority and minority when it comes to decision making. An odd number is always better than even numbers. 1 is also better than 2 control nodes in this case. When a leader control node goes down, the other follower nodes will select a new leader.
Everyone of the control plane node has what is required for when it becomes a leader control node.

Components that make up the Control plane
- api server(kube-apiserver): this is the gateway to the server and the only component we will ever interact directly with. When we send commands to the cluster, it is through the api server and results come from here too
- Cluster store: the only persistent component on the entire Control Plane. The config and state of the cluster are kept here. This is based on the etcd noSQL and super critical for operations. In a large environment, this could easily be the first bottleneck.
- Controller manager: is like a controller of controllers and also watches for changes to make sure the observed state of the cluster matches the desired state. This controls the Node, deployment, and Endpoints, and any other controllers
- Scheduler: watches api server for new tasks and assigns them to nodes.

------Worker Nodes------
This is where we install our apps. Worker nodes can be Linux or Windows
3 worker components we care about:
- Kubelet: this is the main kubernetes agent which registers nodes with clusters. It watches the APi server for work, executes pods, and reports back to the control plane.  
- Container runtime: handles the running of containers with CRI (Container Runtime Interface) as Kubernetes and Kubelets do NOT know how to run containers. New platforms could use containerd. 
- Kube proxy: this is the network brains of the nodes. Makes sure every pod gets IP's and ensure Load balancing works as it should.

*Some cloud providers offer nodeless kubernetes. This is where you can run your containers without thinking about all these things stated. You have your config in your file and the cloud just runs them.

Declarative Model and Desired State - This is key to Kubernetes
Kubernetes runs ar a Declarative model which means we give Kubernetes a configuration of what we want, and it handles the rest for us without us telling it how to achieve our desired state configuration.
The process of telling it how is known as the imperative model which we do NOT use even though Kubernetes supports it as well. Declarative = Declare what you want.
Kubernetes is obsessed with observed state matching desired state so it is always looking to make sure they are in sync with DSC via watch loops.
DSC = Desired State Configuration

------Pods------
Atomic unit of deployment in VMware = Virtual machine
			     Docker = Container
			     Kubernetes = Pod
Pods are the most fundamental element in Kubernetes. This is a shared execution environment. Basically a collection of things an app needs to run. The containers in the pod will share the resources of the pod eg: 2 containers in a pod will share same IP but can have different ports they will communicate with. When you have apps that need to share volumes, we can have them in same pod. 2 containers in the same pod is mostly for specialist use cases though. The unit of scaling in K8s is pods - To scale up, add pods and to scale down, remove pods.
In the pod, there is the main app container, and a helper container. 
All containers in the pod are always scheduled to same cluster node. The term self-healing means a new identical pod is spun up to replace the dead pod but the old pod does NOT come back up.
Pods are a great way to annotate and add labels, add olicies, resources, andd co-scheduling.

------Stable Networking with Services------
Because we cannot rely on pod IP's as they are constantly changing when the pods are replaces, Kubernetes Service objects come into play. The Service sits in front of the pods to provide a stable IP and DNS name, as well as Load balancing. It never changes the stable IP and DNS name. 
	stable IP and DNS name
web --> 	SVC	--> Multiple Pods
	    Load Balancing

Labels: these bring a lot of power and flexibility to Kubernetes. Everything in K8s gets a label. A label can be used to load balance any pods with the soecified label. Once the pod matches the label selector, service will load balance any traffic coming to it.
Services 
- only send traffic to healthy pods
- can do session affinity
- can send traffic outside of cluster
- can do both TCP and UDP

------Deployments------
There is usually a Deployment Controller that watches API server for new deployments to make sure our desired state matches observed state. Once a pod goes down, the controller works to make sure we get back a pod to match the DSC. It is the job of the ReplicaSet to manage the number of replicas with the deployment sitting on top of the ReplicaSet.
Hierarchy
app --> container --> pod(labels, annotations, co-scheduling) --> ReplicaSet (replica count, self-healing, old versions) --> Deployment (Updates, rollbacks...) 

When looking at a deployment, replicas means pods.

We write our manifest, deploy them by throwing it to the api server, the cluster store stores, sceduler deploys pods to house our apps running in containers.

------API and API Server------
Think of the API as a catalogue of features with the definition of how each one works. New API versions will support even more as it revolves over time.
Pod: atomic unit of scheduling
ReplicaSet: replica count
Deployment: Updates and rollbacks
Service: stable networking
These 4 aforementioned are all resources in the API. Some others include Sts, ds, job, cm, cronjob, ep, ing, psp, wrkr, secret.
APi = Core, apps, storage.k8s.io, networking.k8s.io ...

API server is the way we reach and authenticate with the API. kubectl is used to query the API via the api server{}, about the state of the subjects.
The API server is a component of the Kubernetes control plane that exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.
The main implementation of a Kubernetes API server is kube-apiserver. kube-apiserver is designed to scale horizontallyâ€”that is, it scales by deploying more instances. You can run several instances of kube-apiserver and balance traffic between those instances.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++Practice

-----Deploying Pods-----
Think of pods as apps and the nodes as infrastructure where the app runs.
First we build our docker image
# git clone https://github.com/nigelpoulton/getting-started-k8s.git
# cd getting-started-k8s/App/v1  (navigate to where the Dockerfile is)
# docker build -t calvindike/getting-started-k8s:1.0 .
# docker images
# docker push calvindike/getting-started-k8s:1.0 (push image to dockerhub with 1.0 tag)

# vi pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: hello-pod
  labels:
    app: web
spec:
  containers:
    - name: web-ctr
      image: calvindike/getting-started-k8s:1.0
      ports:
        - containerPort: 8080

# kubectl apply -f pod.yml (deploy pod from manifest file)
# kubectl get pods --watch (watch as pod comes up and status changes)
# kubectl get pods -o wide (list pods and node they are running on)
# kubectl describe pod hello-pod (check details of pod)

- We have successfully deployed an application wraped in a container, in a pod.
* Note: kubectl get = one-line info
	kubectl describe = detailed info

# vi multi-pod.yml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: main-ctr
    image: nigelpoulton/nginxadapter:1.0
    ports:
    - containerPort: 80
  - name: helper-ctr
    image: nginx/nginx-prometheus-exporter
    args: ["-nginx.scrape-uri","http://localhost/nginx_status"]
    ports:
    - containerPort: 9113
# kubectl apply -f multi-pod.yml
# kubectl delete -f multi-pod.yml (delete multi container pod)

----Service----
Cluster IP is the IP on the frontend which is automatically assigned by Kubernetes - this is only for use inside the cluster.
Every cluster has a DNS service based on Core DNS. 
Every container in a pod can also resolve any service name because of Core DNS.
Labels are how a service sends traffic to a pod.
*Always make sure your app knows the name of the service it is connecting to.
A service gets a network port which can be mapped on any node. This is a Nodeport as every node has a port

Example of adding NodePort the imperative way instead of declarative way which should be done from our yaml file
# kubectl expose pod hello-pod --name=hello-svc --target-port=8080 --type=NodePort
# kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
hello-svc    NodePort    10.101.51.15   <none>        8080:31093/TCP   3m53s
kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP          10h

In your browser, go to http://localhost:31093/
NodePorts are automatically assigned by Kubernetes from ports 30000-32767
# kubectl delete svc hello-svc (delete NodePort service)

--Declarative Service--
When you don't explicitly name the type of port, you will get the default Cluster IP.
# vi svc-nodeport.yml
apiVersion: v1
kind: Service
metadata:
  name: ps-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 31111
    protocol: TCP
  selector:
    app: web

# kubectl apply -f svc-nodeport.yml
# kubectl describe svc ps-nodeport
# kubectl get ep (get endpoints with same name as our created services)

port = port the service listens on inside the cluster. When an app in the cluster wants to connect to listed service, it will connect via this port.
targetPort = port the app container is listening on.
nodePort = external port that will be mapped on every cluster node.
selector - label selector that has to match the labels of our pod
# kubectl get pods --show-labels

App in container port 8080 --> targetPort 8080 --> Service ClusterIP: 80  --> 
							   NodePort: 31111

- Load balancer
LoadBalancer Service. Will only work on supported cloud platforms (AKS, EKS, GKE, DOK, IBM, LKE etc...) Listens externally on 80 and forwards to Pod/container on 8080
# vi svc-lb.yml
apiVersion: v1
kind: Service
metadata:
  name: ps-lb
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: web
# kubectl apply -f svc-lb.yml

- Deployments
Simple deployment used to deploy and manage the app in nigelpoulton/getting-started-k8s:1.0
# vi deploy-complete
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deploy
  labels:
    app: web
spec:
  replicas: 5
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      terminationGracePeriodSeconds: 1
      containers:
      - name: hello-pod
        image: nigelpoulton/getting-started-k8s:1.0
        imagePullPolicy: Always
        ports:
        - containerPort: 8080

# kubectl get svc
# kubectl get pods
# kubectl get deploy
* Everytime we create a service, we automatically get an endpoint object that hosts a list of healthy pods that match the label selector.

- Rolling updates and rollbacks
Simple deployment used to deploy and manage the app in nigelpoulton/ps-web:1.0
# vi deploy-complete.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deploy
  labels:
    app: web
spec:
  selector:
    matchLabels:
      app: web
  replicas: 5
  minReadySeconds: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  template:
    metadata:
      labels:
        app: web
    spec:
      terminationGracePeriodSeconds: 1
      containers:
      - name: hello-pod
        image: nigelpoulton/getting-started-k8s:2.0
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
# kubectl apply -f deploy-complete.yml
# kubectl rollout status deploy web-deploy
# kubectl get rs (see old and new versions of the pod)
# kubectl rollout history deploy web-deploy (see history)
# kubectl rollout undo deploy web-deploy --to-revision 1 (rollback to first version)
# kubectl get rs

===========================================================================================================================================================================================================
Kubernetes Installation and Configuration Fundamentals - Anthony Nocentino

What is Kubernetes
Kubernetes is a container orchestrator: It starts and stops containers based on the configuration given to it. It also provides infrastructure abstraction which means we tell it what we want and it handles it for us as well as keeping the desired state we want by actively monitoring the containers and changing changing things when needed.

Benefits of K8s
- speed of deployment
- ability to absorb change quickly
- ability to recover quickly (when observed state is different from desired state)
- allow us hide infra complexity in the cluster (developers don't need to worry about a lot when deploying applications)

Kubernetes Principles
- Desired State/Declarative Configuration
- Controllers/Control Loops: constantly monitor and make changes to systems to make sure it's in desired state
- Kubernetes API/API Server: provides a collection of objects that we can use to deploy in code
API objects like pods, controllers, services, and nodes allow us to configure the state of the system declaratively and imperatively.
API server is a Restful API over HTTP using JSON and is the way we interact with our cluster. This is also the only way Kubernetes interacts with our cluster. 

Pods 
A construct that represent one or more containers. This is pretty much our application or service and is the most basic unit of work. Pods are ephemeral and as such, are never redeployed instead, a new pod will be deployed to provide the services the old pod was running. Kubernetes tracks the state of our pods to make sure everything is healthy. It uses probes to do these health checks.

Controllers
Defines our desired state as well as create and manage pods for us. Controllers monitor and respond to the health states of pods according to the result of the probes.
ReplicaSet
This controller allows us to define the number of replicas of a pod we want running at all times.
We will usually create a Deployment instead of a ReplicaSet when deploying an application as a Deployment covers the ReplicaSet configuration and helps us manage rollouts and rollbacks of these ReplicaSets.

Services
These add persistency to our ephemeral world. It helps with network abstraction for pod access, PI and DNS name for the service and is dynamically updated based on the Pod cycle. Serviecs are scaled by adding/removing pods.

* Persistent volume is a pod independent storage. The pod claims this storage that we have configured/set from the cluster.

Cluster components
control plane node: leader
node: worker
Each node in a cluster contributes to the compute capacity of the cluster. Nodes can either be virtual or physical machines.

Components of control plane node
- API server: communication hub of our cluster. This is stateless.
This is central to the control of our cluster. It validates any config coming to the cluster.
- Cluster store(etcd): persists the state of our objects using key-value parirs
- Scheduler: tells K8s which nodes to start pods on based on administrative policies or available resources
Watches API server, schedule pods, resources, and respects constraints.
- Controller Manager: implements lifecycle functions of the controllers that monitor the cluster. Pretty much in charge of desired state
Executes controller loops, watch and update the API server and makes sure correct number of pods or replica sets are running.


Node
This is where our applications run. We can have as many nodes as possible.
Components of nodes
- Kubelet: monitors API server for changes and is responsible for pod lifecycle. This reports Node and Pod state
- kube-proxy: has the responsibility to implement services, route traffic to pods, and load balance.
- Container runtime: resoinsible for downloading images and run containers. Default is containerd but there are many others you can use. Docker used to be the container runtime but was changed in v1.2
These will also run on the control plane nodes, not just the worker nodes.
Kubelet and kube-proxy communicate directly with the API server.

Cluster Add-on Pods
Pods that provide special services to the pod
DNS: uses Core DNS to provide DNS services to the pod
Ingress controllers: for HTTP or layer7 loadbalancer
Dashboard: for web based administration of cluster
Overlays:

Pod Operations
We use kubectl to submit code to Kubernetes via the API server which stores the information in etcd. Scheduler schedules, and controller manager monitors the state after the kubelet receives work from the API server. Control plane nodes will usually only run pods that are system pods.
*Personal note - You can look at things from a node level or from a cluster level.

Kubernetes Networking Requirements
1. Pods on a Node can communicate with all Pods on all Nodes without Network Address Translation (NAT)
2. Agents on a Node can communicate with all Pods on that Node.
In summary, all pods and nodes should be able to connect or talk to each other with the IP addresses they have.












===========================================================================================================================================================================================================
===========================================================================================================================================================================================================
Some distributions of Kubernetes are: 
Amazon - EKS
RedHat - Openshift
VMware - Tanzu
Rancher

Minikube, KIND, K8s are types of Kubernetes development environment which you do NOT use for production. Applications can be tested in this environment before being pushed to production.

Order of most used kubernetes distributions:
Kubernetes
Openshift
Rancher
Tanzu
EKS
AKS
GKE
DKE

If you manually install Kubernetes on servers by yourself, you have no support when you need to fix an issue but with the other distributions tied to a vendor, you can get support depending on the terms of th contract.

Tools to deploy a kubernetes environment: kubeadm, Kops
With kubeadm, you have to do a lot of things manually but it is more automated with Kops.
Kops = Kubernetes Operations

What Kops brings is ease of management though the lifecycle of your cluster in a production environment.
- Install
- Upgrade
- Modifications
- Deletion of clusters










